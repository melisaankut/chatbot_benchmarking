from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import json


def merge_json_and_csv(json_path, csv_path):
    """
    Merges a JSON file (with true answers) and a CSV file (with chatbot responses)
    into a single list of dictionaries in the format:
    {"reference": ..., "generated": ...}
    
    Args:
        json_path (str): Path to the JSON file.
        csv_path (str): Path to the CSV file.
        
    Returns:
        list[dict]: Merged data list.
    """
    # Load JSON
    with open(json_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)

    # Load CSV
    df = pd.read_csv(csv_path)

    # Create a mapping from sentence â†’ response
    response_lookup = dict(zip(df['sentence'], df['response']))

    # Merge based on question_typo (used in both)
    merged = []
    for item in json_data:
        question = item['question']
        reference = item['answer']
        generated = response_lookup.get(question)

        if pd.notna(reference) and pd.notna(generated):  # Filter out NaNs
            merged.append({
                "reference": str(reference).strip(),
                "generated": str(generated).strip()
            })

    return merged

data = merge_json_and_csv("shuffled_100_samples_just_typo.json", "response_texts_typo.csv")
print(data[:5])  # Print first 5 entries for verification
# Load Sentence Embedding model

#embedder = SentenceTransformer('all-MiniLM-L6-v2')
embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def exact_match(ref, gen):
    return int(ref.strip().lower() == gen.strip().lower())

def bleu_score(ref, gen):
    smoothie = SmoothingFunction().method4
    return sentence_bleu([ref.split()], gen.split(), smoothing_function=smoothie)

def rouge_scores(ref, gen):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    return scorer.score(ref, gen)

def cosine_sim(ref, gen):
    embeddings = embedder.encode([ref, gen])
    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

def contains_match(reference, generated):
    """
    Checks whether the reference (ground truth) is a substring of the generated (chatbot) response.

    Args:
        reference (str): The expected correct answer (ground truth).
        generated (str): The chatbot's response.

    Returns:
        int: 1 if reference is found in generated (case-insensitive), else 0.
    """
    if not isinstance(reference, str) or not isinstance(generated, str):
        return 0  # In case of NaN or non-string inputs

    return int(reference.strip().lower() in generated.strip().lower())

def evaluate_example(ref, gen):
    """
    Evaluates a single reference-generated pair using various metrics.
    
    Args:
        ref (str): The reference text.
        gen (str): The generated text by the chatbot.
        
    Returns:
        dict: A dictionary containing all metric scores.
    """
    rouge = rouge_scores(ref, gen)
    return {
        "exact": exact_match(ref, gen),
        "bleu": bleu_score(ref, gen),
        "rouge1": rouge['rouge1'].recall,
        "rougeL": rouge['rougeL'].recall,
        "cosine": cosine_sim(ref, gen),
        "contains": contains_match(ref, gen),
        "hybrid": hybrid_correctness_score(ref, gen)
    }

def hybrid_correctness_score(reference, generated, weight_semantic=0.5, weight_token_f1=0.5):
    """
    Returns a hybrid score combining:
    - Inclusion (reference in generated)
    - Semantic similarity (cosine)
    - Token overlap (F1-style)

    Returns 0 if reference is not present in generated.
    """
    if not isinstance(reference, str) or not isinstance(generated, str):
        return 0.0

    reference = reference.strip().lower()
    generated = generated.strip().lower()

    # 1. Hard constraint: inclusion
    if reference not in generated:
        return 0.0

    # 2. Semantic similarity
    embeddings = embedder.encode([reference, generated])
    cosine_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

    # 3. Token-level overlap
    ref_tokens = set(reference.split())
    gen_tokens = set(generated.split())
    common = ref_tokens.intersection(gen_tokens)

    if not common:
        token_f1 = 0.0
    else:
        precision = len(common) / len(gen_tokens)
        recall = len(common) / len(ref_tokens)
        token_f1 = 2 * precision * recall / (precision + recall)

    # Combine
    return round(weight_semantic * cosine_score + weight_token_f1 * token_f1, 4)


def evaluate_dataset(data):
    """
    Evaluates the full dataset and returns a DataFrame with scores for each example.

    Args:
        data (list): A list of dicts with "reference" and "generated" fields.

    Returns:
        pd.DataFrame: A DataFrame containing all results.
    """
    records = []

    for i, item in enumerate(data):
        ref = item["reference"]
        gen = item["generated"]
        scores = evaluate_example(ref, gen)

        record = {
            "Reference": ref,
            "Generated": gen,
            "Exact Match": scores["exact"],
            "BLEU": scores["bleu"],
            "ROUGE-1": scores["rouge1"],
            "ROUGE-L": scores["rougeL"],
            "Cosine": scores["cosine"],
            "Contains Match": scores["contains"],
            "Hybrid Correctness": scores["hybrid"]
        }
        records.append(record)

    return pd.DataFrame(records)


def print_average_scores(df):
    """
    Prints the average scores across all examples.

    Args:
        df (pd.DataFrame): The evaluation results.
    """
    print("\nAverage Evaluation Scores:")
    for metric in ["Exact Match", "BLEU", "ROUGE-1", "ROUGE-L", "Cosine", "Contains Match", "Hybrid Correctness"]:
        avg = df[metric].mean()
        print(f"{metric:<16}: {avg:.4f}")


def plot_scores(df):
    """
    Plots per-example and average scores.

    Args:
        df (pd.DataFrame): The evaluation results.
    """
    # Line plot per example
    plt.figure(figsize=(10, 6))
    plt.plot(df["BLEU"], label="BLEU")
    plt.plot(df["ROUGE-1"], label="ROUGE-1")
    plt.plot(df["Cosine"], label="Cosine")
    plt.title("Evaluation Metrics Per Example")
    plt.xlabel("Example #")
    plt.ylabel("Score")
    plt.grid(True)
    plt.legend()
    plt.show()

    # Bar plot of averages
    plt.figure(figsize=(8, 5))
    avg_scores = [
        df["Exact Match"].mean(),
        df["BLEU"].mean(),
        df["ROUGE-1"].mean(),
        df["ROUGE-L"].mean(),
        df["Cosine"].mean(),
        df["Contains Match"].mean(),
        df["Hybrid Correctness"].mean()
    ]
    labels = ["Exact", "BLEU", "ROUGE-1", "ROUGE-L", "Cosine", "Contains Match", "Hybrid" ]

    sns.barplot(x=labels, y=avg_scores)
    plt.title("Average Evaluation Scores")
    plt.ylim(0, 1)
    plt.ylabel("Score")
    plt.show()





def evaluate_chatbot_responses(original_csv, typo_csv):
    # Load both CSV files
    df_original = pd.read_csv(original_csv)
    df_typo = pd.read_csv(typo_csv)

    # Normalize the responses
    df_original['response_norm'] = df_original['response'].astype(str).str.strip().str.lower()
    df_typo['response_norm'] = df_typo['response'].astype(str).str.strip().str.lower()

    
    df_result = pd.DataFrame({
        "original_question": df_original['sentence'],
        "typo_question": df_typo['sentence'],
        "original_response": df_original['response'],
        "typo_response": df_typo['response'],
        "match": df_original['response_norm'] == df_typo['response_norm']
    })

    # Calculate accuracy
    total = len(df_result)
    correct = df_result['match'].sum()
    incorrect = total - correct
    accuracy = (correct / total) * 100

    print(f"Total: {total}, Correct: {correct}, Incorrect: {incorrect}, Accuracy: {accuracy:.2f}%")

    # Plot pie chart of matching results
    plt.figure(figsize=(6,6))
    plt.pie([correct, incorrect], labels=['Correct', 'Incorrect'], autopct='%1.1f%%', startangle=90)
    plt.title('Typo vs Original Question Response Accuracy')
    plt.axis('equal')
    plt.show()

    # Optional: Save the comparison results to a CSV
    df_result.to_csv("comparison_result.csv", index=False)

    return df_result


#evaluate_chatbot_responses("shuffled_100_samples_just_typo.json", "response_texts_typo.csv")

# ---- Run the Evaluation ----

results_df = evaluate_dataset(data)
print("Columns in results_df:", results_df.columns.tolist())
print_average_scores(results_df)
plot_scores(results_df)